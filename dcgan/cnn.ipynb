{"cells":[{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["# transform_train = transforms.Compose([transforms.ToTensor()])\n","transform_train = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.137,), (0.3081,))])\n","transform_valid = transforms.Compose([transforms.ToTensor()])\n","\n","trainset = torchvision.datasets.MNIST(root='./data', train=True,\n","                                      download=True, transform=transform_train)#, target_transform=lambda x: torch.Tensor([x]).float())\n","trainloader = DataLoader(trainset, batch_size=4,\n","                         shuffle=True, num_workers=0)\n","\n","testset = torchvision.datasets.MNIST(root='./data', train=False,\n","                                     download=True, transform=transform_valid) #target_transform=lambda x: torch.Tensor([x]).float())\n","testloader = DataLoader(testset, batch_size=20,\n","                        shuffle=False, num_workers=0)\n"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["class Cnn(nn.Module):\n","    def __init__(self):\n","        super(Cnn, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=0)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=0)\n","        self.flatten = nn.Flatten(1)\n","        self.fc1 = nn.Linear(12 * 12 * 64, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        # x = self.pool(x)\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = F.dropout(x, 0.25)\n","        x = self.flatten(x)\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, 0.5)\n","        x = self.fc2(x)\n","        x = F.log_softmax(x, dim=1)\n","        return x\n","\n","metric_func = lambda y_pred, y_true: roc_auc_score(y_true=y_true.data.numpy(), y_score=y_pred.data.numpy())\n","matric_name = \"auc\"\n","def train_step(model: nn.Module, features:torch.Tensor, labels: torch.Tensor):\n","    features = features.to(device=device)\n","    labels = labels.to(device)\n","    \n","    # model.train()\n","    optimizer.zero_grad()\n","    y_pred = model(features)\n","    loss = criterion(y_pred, labels)\n","    # metric = metric_func(y_pred, labels)\n","    metric = torch.Tensor(0)\n","    loss.backward()\n","    optimizer.step()\n","    return loss, 0\n"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","cnn = Cnn()\n","cnn.to(device)\n","#criterion = nn.CrossEntropyLoss()\n","criterion = nn.NLLLoss()\n","optimizer = torch.optim.SGD(cnn.parameters(), lr=0.1)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 26, 26]             320\n","            Conv2d-2           [-1, 64, 24, 24]          18,496\n","         MaxPool2d-3           [-1, 64, 12, 12]               0\n","           Flatten-4                 [-1, 9216]               0\n","            Linear-5                  [-1, 128]       1,179,776\n","            Linear-6                   [-1, 10]           1,290\n","================================================================\n","Total params: 1,199,882\n","Trainable params: 1,199,882\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.002991\n","Forward/backward pass size (MB): 0.587967\n","Params size (MB): 4.577187\n","Estimated Total Size (MB): 5.168144\n","----------------------------------------------------------------\n"]}],"source":["import torchkeras\n","input_shape = (1, 28, 28)\n","torchkeras.summary(Cnn(), input_shape=(1, 28, 28))"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<torch.utils.data.sampler.BatchSampler object at 0x000002997ADC6E08>\n","15000\n"]}],"source":["(features, labels) = next(iter(trainloader))\n","# print((features[0], labels[0]))\n","print(trainloader.batch_sampler)\n","print(len(trainloader))"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,     1] loss: 2.38894\n","[1,    11] loss: 2.40372\n","[1,    21] loss: 2.31416\n","[1,    31] loss: 2.28675\n","[1,    41] loss: 2.21481\n","[1,    51] loss: 2.15304\n","[1,    61] loss: 2.06059\n","[1,    71] loss: 1.97068\n","[1,    81] loss: 1.96126\n","[1,    91] loss: 1.87647\n","[1,   101] loss: 1.83211\n","[1,   111] loss: 1.79312\n","[1,   121] loss: 1.75804\n","[1,   131] loss: 1.71103\n","[1,   141] loss: 1.69429\n","[1,   151] loss: 1.69634\n","[1,   161] loss: 1.68789\n","[1,   171] loss: 1.64760\n","[1,   181] loss: 1.62591\n","[1,   191] loss: 1.58942\n","[1,   201] loss: 1.56226\n","[1,   211] loss: 1.52155\n","[1,   221] loss: 1.51625\n","[1,   231] loss: 1.47697\n","[1,   241] loss: 1.45734\n","[1,   251] loss: 1.44413\n","[1,   261] loss: 1.43355\n","[1,   271] loss: 1.41455\n","[1,   281] loss: 1.39888\n","[1,   291] loss: 1.37891\n","[1,   301] loss: 1.36983\n","[1,   311] loss: 1.35324\n","[1,   321] loss: 1.33943\n","[1,   331] loss: 1.33362\n","[1,   341] loss: 1.32788\n","[1,   351] loss: 1.31338\n","[1,   361] loss: 1.29428\n","[1,   371] loss: 1.27661\n","[1,   381] loss: 1.26332\n","[1,   391] loss: 1.26177\n","[1,   401] loss: 1.25433\n","[1,   411] loss: 1.23685\n","[1,   421] loss: 1.22470\n","[1,   431] loss: 1.22264\n","[1,   441] loss: 1.21907\n","[1,   451] loss: 1.21680\n","[1,   461] loss: 1.20513\n","[1,   471] loss: 1.19807\n","[1,   481] loss: 1.20508\n","[1,   491] loss: 1.19967\n","[1,   501] loss: 1.19083\n","[1,   511] loss: 1.19493\n","[1,   521] loss: 1.19294\n","[1,   531] loss: 1.18231\n","[1,   541] loss: 1.17486\n","[1,   551] loss: 1.16870\n","[1,   561] loss: 1.16044\n","[1,   571] loss: 1.15466\n","[1,   581] loss: 1.15310\n","[1,   591] loss: 1.14727\n","[1,   601] loss: 1.14238\n","[1,   611] loss: 1.13821\n","[1,   621] loss: 1.13241\n","[1,   631] loss: 1.12500\n","[1,   641] loss: 1.13298\n","[1,   651] loss: 1.13284\n","[1,   661] loss: 1.12810\n","[1,   671] loss: 1.12806\n","[1,   681] loss: 1.12349\n","[1,   691] loss: 1.12025\n","[1,   701] loss: 1.12118\n","[1,   711] loss: 1.11616\n","[1,   721] loss: 1.11330\n","[1,   731] loss: 1.10979\n","[1,   741] loss: 1.10052\n","[1,   751] loss: 1.09119\n","[1,   761] loss: 1.08180\n","[1,   771] loss: 1.07934\n","[1,   781] loss: 1.07393\n","[1,   791] loss: 1.07123\n","[1,   801] loss: 1.06707\n","[1,   811] loss: 1.06272\n","[1,   821] loss: 1.05820\n","[1,   831] loss: 1.05143\n","[1,   841] loss: 1.04316\n","[1,   851] loss: 1.04393\n","[1,   861] loss: 1.03545\n","[1,   871] loss: 1.02691\n","[1,   881] loss: 1.02082\n","[1,   891] loss: 1.01824\n","[1,   901] loss: 1.01304\n","[1,   911] loss: 1.01526\n","[1,   921] loss: 1.00966\n","[1,   931] loss: 1.01121\n","[1,   941] loss: 1.01037\n","[1,   951] loss: 1.00448\n","[1,   961] loss: 1.00146\n","[1,   971] loss: 0.99873\n","[1,   981] loss: 0.99449\n","[1,   991] loss: 0.99013\n","[1,  1001] loss: 0.98544\n","[1,  1011] loss: 0.98803\n","[1,  1021] loss: 0.98509\n","[1,  1031] loss: 0.98457\n","[1,  1041] loss: 0.98162\n","[1,  1051] loss: 0.98168\n","[1,  1061] loss: 0.98015\n","[1,  1071] loss: 0.97910\n","[1,  1081] loss: 0.97618\n","[1,  1091] loss: 0.97348\n","[1,  1101] loss: 0.97015\n","[1,  1111] loss: 0.96884\n","[1,  1121] loss: 0.96871\n","[1,  1131] loss: 0.96825\n","[1,  1141] loss: 0.96436\n","[1,  1151] loss: 0.96146\n","[1,  1161] loss: 0.96267\n","[1,  1171] loss: 0.95983\n","[1,  1181] loss: 0.95674\n","[1,  1191] loss: 0.95285\n","[1,  1201] loss: 0.95083\n","[1,  1211] loss: 0.94962\n","[1,  1221] loss: 0.94524\n","[1,  1231] loss: 0.94148\n","[1,  1241] loss: 0.93881\n","[1,  1251] loss: 0.93624\n","[1,  1261] loss: 0.93490\n","[1,  1271] loss: 0.93178\n","[1,  1281] loss: 0.92697\n","[1,  1291] loss: 0.92325\n","[1,  1301] loss: 0.91861\n","[1,  1311] loss: 0.91702\n","[1,  1321] loss: 0.91566\n","[1,  1331] loss: 0.91327\n","[1,  1341] loss: 0.90990\n","[1,  1351] loss: 0.90695\n","[1,  1361] loss: 0.90422\n","[1,  1371] loss: 0.90004\n","[1,  1381] loss: 0.90232\n","[1,  1391] loss: 0.89971\n","[1,  1401] loss: 0.89848\n","[1,  1411] loss: 0.89744\n","[1,  1421] loss: 0.89394\n","[1,  1431] loss: 0.89118\n","[1,  1441] loss: 0.89095\n","[1,  1451] loss: 0.88818\n","[1,  1461] loss: 0.88797\n","[1,  1471] loss: 0.88700\n","[1,  1481] loss: 0.88412\n","[1,  1491] loss: 0.88252\n","[1,  1501] loss: 0.88077\n","[1,  1511] loss: 0.87952\n","[1,  1521] loss: 0.87635\n","[1,  1531] loss: 0.87231\n","[1,  1541] loss: 0.86883\n","[1,  1551] loss: 0.87197\n","[1,  1561] loss: 0.86873\n","[1,  1571] loss: 0.86475\n","[1,  1581] loss: 0.86136\n","[1,  1591] loss: 0.85720\n","[1,  1601] loss: 0.85886\n","[1,  1611] loss: 0.85562\n","[1,  1621] loss: 0.85478\n","[1,  1631] loss: 0.85381\n","[1,  1641] loss: 0.85364\n","[1,  1651] loss: 0.85473\n","[1,  1661] loss: 0.85173\n","[1,  1671] loss: 0.84894\n","[1,  1681] loss: 0.84595\n","[1,  1691] loss: 0.84296\n","[1,  1701] loss: 0.84335\n","[1,  1711] loss: 0.84084\n","[1,  1721] loss: 0.83969\n","[1,  1731] loss: 0.83953\n","[1,  1741] loss: 0.83898\n","[1,  1751] loss: 0.83761\n","[1,  1761] loss: 0.83592\n","[1,  1771] loss: 0.83460\n","[1,  1781] loss: 0.83841\n","[1,  1791] loss: 0.83798\n","[1,  1801] loss: 0.83575\n","[1,  1811] loss: 0.83366\n","[1,  1821] loss: 0.83096\n","[1,  1831] loss: 0.82824\n","[1,  1841] loss: 0.82748\n","[1,  1851] loss: 0.82612\n","[1,  1861] loss: 0.82328\n","[1,  1871] loss: 0.82760\n","[1,  1881] loss: 0.82571\n","[1,  1891] loss: 0.82404\n","[1,  1901] loss: 0.82147\n","[1,  1911] loss: 0.82052\n","[1,  1921] loss: 0.82068\n","[1,  1931] loss: 0.82383\n","[1,  1941] loss: 0.82216\n","[1,  1951] loss: 0.81969\n","[1,  1961] loss: 0.81746\n","[1,  1971] loss: 0.81681\n","[1,  1981] loss: 0.81736\n","[1,  1991] loss: 0.81619\n","[1,  2001] loss: 0.81367\n","[1,  2011] loss: 0.81104\n","[1,  2021] loss: 0.80988\n","[1,  2031] loss: 0.80793\n","[1,  2041] loss: 0.80680\n","[1,  2051] loss: 0.80529\n","[1,  2061] loss: 0.80421\n","[1,  2071] loss: 0.80508\n","[1,  2081] loss: 0.80623\n","[1,  2091] loss: 0.80450\n","[1,  2101] loss: 0.80416\n","[1,  2111] loss: 0.80487\n","[1,  2121] loss: 0.80234\n","[1,  2131] loss: 0.80118\n","[1,  2141] loss: 0.79982\n","[1,  2151] loss: 0.79924\n","[1,  2161] loss: 0.79984\n","[1,  2171] loss: 0.79750\n","[1,  2181] loss: 0.79801\n","[1,  2191] loss: 0.79534\n","[1,  2201] loss: 0.79566\n","[1,  2211] loss: 0.79401\n","[1,  2221] loss: 0.79332\n","[1,  2231] loss: 0.79323\n","[1,  2241] loss: 0.79185\n","[1,  2251] loss: 0.79011\n","[1,  2261] loss: 0.78936\n","[1,  2271] loss: 0.78737\n","[1,  2281] loss: 0.78802\n","[1,  2291] loss: 0.78696\n","[1,  2301] loss: 0.78435\n","[1,  2311] loss: 0.78301\n","[1,  2321] loss: 0.78702\n","[1,  2331] loss: 0.79047\n","[1,  2341] loss: 0.79031\n","[1,  2351] loss: 0.78945\n","[1,  2361] loss: 0.78884\n","[1,  2371] loss: 0.78642\n","[1,  2381] loss: 0.78608\n","[1,  2391] loss: 0.78425\n","[1,  2401] loss: 0.78382\n","[1,  2411] loss: 0.78256\n","[1,  2421] loss: 0.78211\n","[1,  2431] loss: 0.78141\n","[1,  2441] loss: 0.78215\n","[1,  2451] loss: 0.78372\n","[1,  2461] loss: 0.78353\n","[1,  2471] loss: 0.78200\n","[1,  2481] loss: 0.78312\n","[1,  2491] loss: 0.78442\n","[1,  2501] loss: 0.78645\n","[1,  2511] loss: 0.79000\n","[1,  2521] loss: 0.78948\n","[1,  2531] loss: 0.78914\n","[1,  2541] loss: 0.78919\n","[1,  2551] loss: 0.79425\n","[1,  2561] loss: 0.79366\n","[1,  2571] loss: 0.79204\n","[1,  2581] loss: 0.79236\n","[1,  2591] loss: 0.79106\n","[1,  2601] loss: 0.79187\n","[1,  2611] loss: 0.79078\n","[1,  2621] loss: 0.79045\n","[1,  2631] loss: 0.78906\n","[1,  2641] loss: 0.78804\n","[1,  2651] loss: 0.78697\n","[1,  2661] loss: 0.78757\n","[1,  2671] loss: 0.78614\n","[1,  2681] loss: 0.78556\n","[1,  2691] loss: 0.78482\n","[1,  2701] loss: 0.78424\n","[1,  2711] loss: 0.78411\n","[1,  2721] loss: 0.78532\n","[1,  2731] loss: 0.78433\n","[1,  2741] loss: 0.78476\n","[1,  2751] loss: 0.78417\n","[1,  2761] loss: 0.78347\n","[1,  2771] loss: 0.78277\n","[1,  2781] loss: 0.78261\n","[1,  2791] loss: 0.78440\n","[1,  2801] loss: 0.78291\n","[1,  2811] loss: 0.78219\n","[1,  2821] loss: 0.78093\n","[1,  2831] loss: 0.78065\n","[1,  2841] loss: 0.77926\n","[1,  2851] loss: 0.77843\n","[1,  2861] loss: 0.77786\n","[1,  2871] loss: 0.77651\n","[1,  2881] loss: 0.77485\n","[1,  2891] loss: 0.77416\n","[1,  2901] loss: 0.77443\n","[1,  2911] loss: 0.77393\n","[1,  2921] loss: 0.77367\n","[1,  2931] loss: 0.77323\n","[1,  2941] loss: 0.77235\n","[1,  2951] loss: 0.77226\n","[1,  2961] loss: 0.77063\n","[1,  2971] loss: 0.77143\n","[1,  2981] loss: 0.77087\n","[1,  2991] loss: 0.76981\n","[1,  3001] loss: 0.76876\n","[1,  3011] loss: 0.76780\n","[1,  3021] loss: 0.76644\n","[1,  3031] loss: 0.76471\n","[1,  3041] loss: 0.76486\n","[1,  3051] loss: 0.76475\n","[1,  3061] loss: 0.76307\n","[1,  3071] loss: 0.76282\n","[1,  3081] loss: 0.76061\n","[1,  3091] loss: 0.76074\n","[1,  3101] loss: 0.76190\n","[1,  3111] loss: 0.76132\n","[1,  3121] loss: 0.76020\n","[1,  3131] loss: 0.75880\n","[1,  3141] loss: 0.75969\n","[1,  3151] loss: 0.75985\n","[1,  3161] loss: 0.75867\n","[1,  3171] loss: 0.75778\n","[1,  3181] loss: 0.75751\n","[1,  3191] loss: 0.75658\n","[1,  3201] loss: 0.75573\n","[1,  3211] loss: 0.75464\n","[1,  3221] loss: 0.75420\n","[1,  3231] loss: 0.75340\n","[1,  3241] loss: 0.75297\n","[1,  3251] loss: 0.75365\n","[1,  3261] loss: 0.75416\n","[1,  3271] loss: 0.75500\n","[1,  3281] loss: 0.75614\n","[1,  3291] loss: 0.75508\n","[1,  3301] loss: 0.75480\n","[1,  3311] loss: 0.75399\n","[1,  3321] loss: 0.75422\n","[1,  3331] loss: 0.75297\n","[1,  3341] loss: 0.75274\n","[1,  3351] loss: 0.75123\n","[1,  3361] loss: 0.74967\n","[1,  3371] loss: 0.74934\n","[1,  3381] loss: 0.74876\n","[1,  3391] loss: 0.74855\n","[1,  3401] loss: 0.74809\n","[1,  3411] loss: 0.74748\n","[1,  3421] loss: 0.74683\n","[1,  3431] loss: 0.74721\n","[1,  3441] loss: 0.74753\n","[1,  3451] loss: 0.74873\n","[1,  3461] loss: 0.75023\n","[1,  3471] loss: 0.74943\n","[1,  3481] loss: 0.74845\n","[1,  3491] loss: 0.74827\n","[1,  3501] loss: 0.74777\n","[1,  3511] loss: 0.74743\n","[1,  3521] loss: 0.74683\n","[1,  3531] loss: 0.74550\n","[1,  3541] loss: 0.74541\n","[1,  3551] loss: 0.74479\n","[1,  3561] loss: 0.74495\n","[1,  3571] loss: 0.74415\n","[1,  3581] loss: 0.74530\n","[1,  3591] loss: 0.74444\n","[1,  3601] loss: 0.74396\n","[1,  3611] loss: 0.74344\n","[1,  3621] loss: 0.74238\n","[1,  3631] loss: 0.74126\n","[1,  3641] loss: 0.74123\n","[1,  3651] loss: 0.74111\n","[1,  3661] loss: 0.74087\n","[1,  3671] loss: 0.74041\n","[1,  3681] loss: 0.74071\n","[1,  3691] loss: 0.74022\n","[1,  3701] loss: 0.73945\n","[1,  3711] loss: 0.73837\n","[1,  3721] loss: 0.73744\n","[1,  3731] loss: 0.73721\n","[1,  3741] loss: 0.73754\n","[1,  3751] loss: 0.73711\n","[1,  3761] loss: 0.73648\n","[1,  3771] loss: 0.73540\n","[1,  3781] loss: 0.73483\n","[1,  3791] loss: 0.73701\n","[1,  3801] loss: 0.73666\n","[1,  3811] loss: 0.73588\n","[1,  3821] loss: 0.73568\n","[1,  3831] loss: 0.73515\n","[1,  3841] loss: 0.73496\n","[1,  3851] loss: 0.73497\n","[1,  3861] loss: 0.73412\n","[1,  3871] loss: 0.73321\n","[1,  3881] loss: 0.73275\n","[1,  3891] loss: 0.73177\n","[1,  3901] loss: 0.73077\n","[1,  3911] loss: 0.73095\n","[1,  3921] loss: 0.73039\n","[1,  3931] loss: 0.73008\n","[1,  3941] loss: 0.73000\n","[1,  3951] loss: 0.72984\n","[1,  3961] loss: 0.72947\n","[1,  3971] loss: 0.72838\n","[1,  3981] loss: 0.72850\n","[1,  3991] loss: 0.72847\n","[1,  4001] loss: 0.72831\n","[1,  4011] loss: 0.72821\n","[1,  4021] loss: 0.72660\n","[1,  4031] loss: 0.72526\n","[1,  4041] loss: 0.72512\n","[1,  4051] loss: 0.72601\n","[1,  4061] loss: 0.72515\n","[1,  4071] loss: 0.72569\n","[1,  4081] loss: 0.72551\n","[1,  4091] loss: 0.72498\n","[1,  4101] loss: 0.72422\n","[1,  4111] loss: 0.72335\n","[1,  4121] loss: 0.72221\n","[1,  4131] loss: 0.72285\n","[1,  4141] loss: 0.72233\n","[1,  4151] loss: 0.72224\n","[1,  4161] loss: 0.72200\n","[1,  4171] loss: 0.72071\n","[1,  4181] loss: 0.72000\n","[1,  4191] loss: 0.71870\n","[1,  4201] loss: 0.71851\n","[1,  4211] loss: 0.71791\n","[1,  4221] loss: 0.71720\n","[1,  4231] loss: 0.71620\n","[1,  4241] loss: 0.71496\n","[1,  4251] loss: 0.71610\n","[1,  4261] loss: 0.71626\n","[1,  4271] loss: 0.71592\n","[1,  4281] loss: 0.71544\n","[1,  4291] loss: 0.71483\n","[1,  4301] loss: 0.71414\n","[1,  4311] loss: 0.71439\n","[1,  4321] loss: 0.71326\n","[1,  4331] loss: 0.71236\n","[1,  4341] loss: 0.71155\n","[1,  4351] loss: 0.71120\n","[1,  4361] loss: 0.71117\n","[1,  4371] loss: 0.71088\n","[1,  4381] loss: 0.71050\n","[1,  4391] loss: 0.71006\n","[1,  4401] loss: 0.70872\n","[1,  4411] loss: 0.70795\n","[1,  4421] loss: 0.70798\n","[1,  4431] loss: 0.70846\n","[1,  4441] loss: 0.70834\n","[1,  4451] loss: 0.70852\n","[1,  4461] loss: 0.70883\n","[1,  4471] loss: 0.70843\n","[1,  4481] loss: 0.70843\n","[1,  4491] loss: 0.70896\n","[1,  4501] loss: 0.70788\n","[1,  4511] loss: 0.70790\n","[1,  4521] loss: 0.70772\n","[1,  4531] loss: 0.70720\n","[1,  4541] loss: 0.70772\n","[1,  4551] loss: 0.70780\n","[1,  4561] loss: 0.70806\n","[1,  4571] loss: 0.70740\n","[1,  4581] loss: 0.70715\n","[1,  4591] loss: 0.70696\n","[1,  4601] loss: 0.70590\n","[1,  4611] loss: 0.70564\n","[1,  4621] loss: 0.70543\n","[1,  4631] loss: 0.70421\n","[1,  4641] loss: 0.70357\n","[1,  4651] loss: 0.70304\n","[1,  4661] loss: 0.70540\n","[1,  4671] loss: 0.70558\n","[1,  4681] loss: 0.70597\n","[1,  4691] loss: 0.70575\n","[1,  4701] loss: 0.70540\n","[1,  4711] loss: 0.70522\n","[1,  4721] loss: 0.70434\n","[1,  4731] loss: 0.70608\n","[1,  4741] loss: 0.70581\n","[1,  4751] loss: 0.70652\n","[1,  4761] loss: 0.70612\n","[1,  4771] loss: 0.70590\n","[1,  4781] loss: 0.70612\n","[1,  4791] loss: 0.70656\n","[1,  4801] loss: 0.70628\n","[1,  4811] loss: 0.70564\n","[1,  4821] loss: 0.70510\n","[1,  4831] loss: 0.70410\n","[1,  4841] loss: 0.70403\n","[1,  4851] loss: 0.70405\n","[1,  4861] loss: 0.70336\n","[1,  4871] loss: 0.70244\n","[1,  4881] loss: 0.70174\n","[1,  4891] loss: 0.70182\n","[1,  4901] loss: 0.70138\n","[1,  4911] loss: 0.70074\n","[1,  4921] loss: 0.69972\n","[1,  4931] loss: 0.69873\n","[1,  4941] loss: 0.69833\n","[1,  4951] loss: 0.69848\n","[1,  4961] loss: 0.69898\n","[1,  4971] loss: 0.69798\n","[1,  4981] loss: 0.69765\n","[1,  4991] loss: 0.69857\n","[1,  5001] loss: 0.69948\n","[1,  5011] loss: 0.69891\n","[1,  5021] loss: 0.69881\n","[1,  5031] loss: 0.69827\n","[1,  5041] loss: 0.69907\n","[1,  5051] loss: 0.70059\n","[1,  5061] loss: 0.70150\n","[1,  5071] loss: 0.70139\n","[1,  5081] loss: 0.70211\n","[1,  5091] loss: 0.70218\n","[1,  5101] loss: 0.70206\n","[1,  5111] loss: 0.70157\n","[1,  5121] loss: 0.70272\n","[1,  5131] loss: 0.70285\n","[1,  5141] loss: 0.70284\n","[1,  5151] loss: 0.70367\n","[1,  5161] loss: 0.70298\n","[1,  5171] loss: 0.70428\n","[1,  5181] loss: 0.70445\n","[1,  5191] loss: 0.70413\n","[1,  5201] loss: 0.70655\n","[1,  5211] loss: 0.70738\n","[1,  5221] loss: 0.70646\n","[1,  5231] loss: 0.70731\n","[1,  5241] loss: 0.70862\n","[1,  5251] loss: 0.71032\n","[1,  5261] loss: 0.71065\n","[1,  5271] loss: 0.71018\n","[1,  5281] loss: 0.70983\n","[1,  5291] loss: 0.71038\n","[1,  5301] loss: 0.71027\n","[1,  5311] loss: 0.71044\n","[1,  5321] loss: 0.71105\n","[1,  5331] loss: 0.71066\n","[1,  5341] loss: 0.71017\n","[1,  5351] loss: 0.71068\n","[1,  5361] loss: 0.71034\n","[1,  5371] loss: 0.71046\n","[1,  5381] loss: 0.71144\n","[1,  5391] loss: 0.71102\n","[1,  5401] loss: 0.71159\n","[1,  5411] loss: 0.71122\n","[1,  5421] loss: 0.71082\n","[1,  5431] loss: 0.71029\n","[1,  5441] loss: 0.70993\n","[1,  5451] loss: 0.71039\n","[1,  5461] loss: 0.70972\n","[1,  5471] loss: 0.70931\n","[1,  5481] loss: 0.71017\n","[1,  5491] loss: 0.71090\n","[1,  5501] loss: 0.71041\n","[1,  5511] loss: 0.70949\n","[1,  5521] loss: 0.70914\n","[1,  5531] loss: 0.70870\n","[1,  5541] loss: 0.70878\n","[1,  5551] loss: 0.71140\n","[1,  5561] loss: 0.71099\n","[1,  5571] loss: 0.71093\n","[1,  5581] loss: 0.71079\n","[1,  5591] loss: 0.71082\n","[1,  5601] loss: 0.71066\n","[1,  5611] loss: 0.71009\n","[1,  5621] loss: 0.70982\n","[1,  5631] loss: 0.71078\n","[1,  5641] loss: 0.71018\n","[1,  5651] loss: 0.71004\n","[1,  5661] loss: 0.71036\n","[1,  5671] loss: 0.70993\n","[1,  5681] loss: 0.71007\n","[1,  5691] loss: 0.70997\n","[1,  5701] loss: 0.71054\n","[1,  5711] loss: 0.71140\n","[1,  5721] loss: 0.71120\n","[1,  5731] loss: 0.71092\n","[1,  5741] loss: 0.71061\n","[1,  5751] loss: 0.70985\n","[1,  5761] loss: 0.70950\n"]}],"source":["for epoch in range(20):\n","    loss_sum = []\n","    metric_sum = 0.0\n","    total_loss = 0.\n","    for i, data in enumerate(trainloader):\n","        (features, labels) = data\n","        loss, metric = train_step(cnn, features=features, labels=labels)\n","        loss_sum.append(loss)\n","        metric_sum += metric\n","        total_loss += loss.item()\n","        loss_sum.append(loss.item())\n","        loss.retain_grad()\n","        if i % 10 == 0:\n","            print('[%d, %5d] loss: %.5f' %\n","                    (epoch + 1, i + 1, torch.mean(torch.Tensor(loss_sum))))\n","            total_loss = 0.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"ecae59e36edac920ceb43795ff371fc2cc92248ebecf572615877361fb7f6b56"},"kernelspec":{"display_name":"Python 3.7.9 64-bit ('deeplab': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
